{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:45.149011Z","iopub.execute_input":"2021-11-19T18:34:45.149324Z","iopub.status.idle":"2021-11-19T18:34:45.361431Z","shell.execute_reply.started":"2021-11-19T18:34:45.149235Z","shell.execute_reply":"2021-11-19T18:34:45.360651Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%time\nimport pandas as pd\nimport glob, os\n\ntrain_target = pd.read_csv('../input/lifeqhack/training_target.csv')\nfiles_train = glob.glob('../input/lifeqhack/train/train/*.csv')\nfiles_test = glob.glob('../input/lifeqhack/test/test/*.csv')\ntrain = pd.concat([pd.read_csv(i).assign(ID=os.path.basename(i)) for i in files_train])\ntest = pd.concat([pd.read_csv(i).assign(ID=os.path.basename(i)) for i in files_test])\ntrain['ID'] = train['ID'].apply(lambda x: x.split(\".\")[0])\ntest['ID'] = test['ID'].apply(lambda x: x.split(\".\")[0])\ntrain = train.merge(train_target, on = 'ID', how = 'left')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:34:52.851263Z","iopub.execute_input":"2021-11-19T18:34:52.851963Z","iopub.status.idle":"2021-11-19T18:35:10.487796Z","shell.execute_reply.started":"2021-11-19T18:34:52.851910Z","shell.execute_reply":"2021-11-19T18:35:10.486962Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(by=['rr_interval'])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T10:45:44.209763Z","iopub.execute_input":"2021-11-19T10:45:44.210026Z","iopub.status.idle":"2021-11-19T10:45:45.315968Z","shell.execute_reply.started":"2021-11-19T10:45:44.209991Z","shell.execute_reply":"2021-11-19T10:45:45.315095Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# test[test[\"rr_interval\"].isin(train[\"rr_interval\"])]\n# test[['ms',\"rr_interval\"]][~ test[['ms',\"rr_interval\"]].isin(train[['ms',\"rr_interval\"]])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg = train.drop(['target'],axis=1).groupby('ID', as_index = False).agg(\n    ['max','min','mean','count','std','median']).reset_index()\ncolumns = ['ID']\n\n# Iterate through the variables names\nfor var in agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'ID':\n        \n        # Iterate through the stat names\n        for stat in agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('agg_%s_%s' % (var, stat))\nagg.columns = columns\ntrain = train.merge(agg, on = 'ID', how = 'left')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:35:10.489615Z","iopub.execute_input":"2021-11-19T18:35:10.489894Z","iopub.status.idle":"2021-11-19T18:35:17.357437Z","shell.execute_reply.started":"2021-11-19T18:35:10.489858Z","shell.execute_reply":"2021-11-19T18:35:17.356747Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"agg = test.groupby('ID', as_index = False).agg(\n    ['max','min','mean','count','std','median']).reset_index()\ncolumns = ['ID']\n\n# Iterate through the variables names\nfor var in agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'ID':\n        \n        # Iterate through the stat names\n        for stat in agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('agg_%s_%s' % (var, stat))\nagg.columns = columns\ntest = test.merge(agg, on = 'ID', how = 'left')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:35:25.351253Z","iopub.execute_input":"2021-11-19T18:35:25.351634Z","iopub.status.idle":"2021-11-19T18:35:28.507287Z","shell.execute_reply.started":"2021-11-19T18:35:25.351601Z","shell.execute_reply":"2021-11-19T18:35:28.506622Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#this may crush the ram\n# agg = train.drop(['ID'],axis=1).groupby('rr_interval', as_index = False).agg(\n#     ['max','min','mean','count']).reset_index()\n# columns = ['rr_interval']\n\n# # Iterate through the variables names\n# for var in agg.columns.levels[0]:\n#     # Skip the id name\n#     if var != 'rr_interval':\n        \n#         # Iterate through the stat names\n#         for stat in agg.columns.levels[1][:-1]:\n#             # Make a new column name for the variable and stat\n#             columns.append('agg_%s_%s' % (var, stat))\n# agg.columns = columns\n# train = train.merge(agg, on = 'rr_interval', how = 'left')\n# test = test.merge(agg, on = 'rr_interval', how = 'left')\n# train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import roc_auc_score\n\n# train_1 = train[train['target']!=1]\nX = train.drop(['ID', 'target'],axis=1)\ny = train['target']\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.7, random_state=1, \n                                                      shuffle=True) #random_state=0","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:35:53.683392Z","iopub.execute_input":"2021-11-19T18:35:53.683684Z","iopub.status.idle":"2021-11-19T18:35:56.942132Z","shell.execute_reply.started":"2021-11-19T18:35:53.683654Z","shell.execute_reply":"2021-11-19T18:35:56.941413Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\ncatboost = CatBoostClassifier(iterations=10 ,\n                              task_type=\"GPU\"\n                              ,depth=10) #   task_type=\"GPU\",  bagging_temperature=10 80%\ncatboost.fit(X_train, y_train)\npredcatboost = catboost.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:39:06.984113Z","iopub.execute_input":"2021-11-19T18:39:06.984463Z","iopub.status.idle":"2021-11-19T18:39:15.257380Z","shell.execute_reply.started":"2021-11-19T18:39:06.984425Z","shell.execute_reply":"2021-11-19T18:39:15.256591Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_valid, predcatboost)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:39:15.258826Z","iopub.execute_input":"2021-11-19T18:39:15.259422Z","iopub.status.idle":"2021-11-19T18:39:15.609692Z","shell.execute_reply.started":"2021-11-19T18:39:15.259379Z","shell.execute_reply":"2021-11-19T18:39:15.609027Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgb\n\n\nlightgbm = lgb.LGBMClassifier(device = 'gpu')\nlightgbm.fit(X_train, y_train)\npredlightgbm = lightgbm.predict(X_valid)\n\nxgb = xgb.XGBClassifier(n_estimators=500,\n                        max_depth=9,\n                        learning_rate=0.05)\n\nxgb.fit(X_train, y_train)\npredxgb = xgb.predict(X_valid)\naccuracy_score(y_valid, predxgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0 = 136\n# 1=7\n# 2=23\ntest_ = test\ntest_['target'] = catboost.predict(test_.drop(['ID'],axis=1))\ntest_ = test_.drop_duplicates(subset=[\"ID\"])\ntest_[['ID', 'target']].to_csv('johannes_new.csv',index=False)\ntest_['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:39:21.944615Z","iopub.execute_input":"2021-11-19T18:39:21.944897Z","iopub.status.idle":"2021-11-19T18:39:25.545504Z","shell.execute_reply.started":"2021-11-19T18:39:21.944867Z","shell.execute_reply":"2021-11-19T18:39:25.544781Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# def rules(x):\n#     if x['target']==2:\n#         return 1\n#     else:\n#         return 0\n    \n\n# test_['target'] = model.predict(test_.drop(['ID'],axis=1))\n# # test_['model_2'] = model_2.predict(test_.drop(['ID'],axis=1))\n# test_ = test_.drop_duplicates(subset=[\"ID\"])\n# test_['target'] = test_.apply(rules, axis=1)\n# test_['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}